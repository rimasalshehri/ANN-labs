{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Q1\nimport math\nimport numpy as np\n\ndef single_input_neuron(p, w, b, f):\n    # Calculate the output of the summation\n    n = w * p + b\n    \n    # Apply the transfer function\n    if f == 'linear':\n        return n  # Linear transfer function: output is the same as the summation\n    elif f == 'logsigmoid':\n        return 1.0 / (1 + np.exp(-n))  # Sigmoid transfer function: output is the sigmoid of the summation\n    elif f == 'relu':\n        return np.where(n < 0, 0 , n)  # ReLU transfer function: output is the maximum of 0 and the summation\n    elif f == 'hardlimit':\n        return np.where(n >= 0, 1 , 0)  # Hardlimit transfer function: output is 1 if n is greater than or equal to 0, otherwise 0\n    elif f == 'tanh':\n        return (np.exp(n) - np.exp(-n)) / (np.exp(n) + np.exp(-n))  # Tanh transfer function\n    else:\n        raise ValueError(\"Invalid transfer function. Please choose 'linear', 'logsigmoid', 'relu', 'hardlimit', or 'tanh'.\")\n\n# Ask the user for inputs\np = float(input(\"Enter the input value (p): \"))\nw = float(input(\"Enter the weight (w): \"))\nb = float(input(\"Enter the bias (b): \"))\n\nwhile True:\n    f = input(\"Enter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate: \")\n    if f == \"\":\n        break\n    \n    try:\n        output = single_input_neuron(p, w, b, f)\n        print(\"Output (a):\", output)\n    except ValueError as e:\n        print(\"Error:\", str(e))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T17:16:09.151836Z","iopub.execute_input":"2023-09-14T17:16:09.152225Z","iopub.status.idle":"2023-09-14T17:16:38.895962Z","shell.execute_reply.started":"2023-09-14T17:16:09.152195Z","shell.execute_reply":"2023-09-14T17:16:38.895204Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the input value (p):  2\nEnter the weight (w):  -1.5\nEnter the bias (b):  3\nEnter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate:  hardlimit\n"},{"name":"stdout","text":"Output (a): 1\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate:  relu\n"},{"name":"stdout","text":"Output (a): 0.0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate:  linear\n"},{"name":"stdout","text":"Output (a): 0.0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate:  logsigmoid\n"},{"name":"stdout","text":"Output (a): 0.5\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate:  tanh\n"},{"name":"stdout","text":"Output (a): 0.0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter the transfer function (linear, logsigmoid, relu, hardlimit, or tanh), or press Enter to terminate:  \n"}]},{"cell_type":"markdown","source":"# Check if each of the three answers that you have given above are correct? for Q1\n# a=f(wp+b)\n# p=2 , w=-1.5 , b=3**\n\n----------------------------------------------------------------------------------\n\n**1-hardlimit funcrion-->\na=hardlimit(-1.5*2+3) \na=hardlimit(-3+3)\na=hardlimit(0) >=0 equals to 1**\n\n---------------------------------------------------------------------------------\n\n**2-logsigmoid function-->\na=logsigmoid(-1.5*2+3)\na=logsigmoid(-3+3)\na=logsigmoid(0) we apply the value 0 which is n to the logsigmoid function\nlogsigmoid=1/(1+e^-n)=1/(1+e^0)=0.5 so n=0.5 and the function =0.5**\n\n----------------------------------------------------------------------------------\n\n\n**3-poslin'Relu' function-->\nposlin function-->\na=poslin(-1.5*2+3)\na=poslin(-3+3)\na=poslin(0) =0 since the condition is  n < 0, relu(n) = 0**\n\n\n----------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Q2 Develop a multiple input neuron. Assuming that it can take as input a column vector p , weight matrix w, bias vector b and produces a summation output n which when passed to the transfer function f \n# generates the output a.    ","metadata":{}},{"cell_type":"code","source":"def neuron(p, w, b, f):\n    wp = np.dot(w, p)\n    n = wp + b\n\n    if f == 'linear':\n        return n  # Linear transfer function: output is the same as the summation\n    elif f == 'logsigmoid':\n        return 1.0 / (1 + np.exp(-n))  # Sigmoid transfer function: output is the sigmoid of the summation\n    elif f == 'relu':\n        return np.maximum(0, n)  # ReLU transfer function: output is the maximum of 0 and the summation\n    elif f == 'hardlimit':\n        return np.where(n >= 0, 1, 0)  # Hardlimit transfer function: output is 1 if n is greater than or equal to 0, otherwise 0\n    elif f == 'tanh':\n        return (np.exp(n) - np.exp(-n)) / (np.exp(n) + np.exp(-n))\n    else:\n        raise ValueError(\"Invalid transfer function. Please choose 'linear', 'logsigmoid', 'relu', 'hardlimit', or 'tanh'.\")\n\np = np.array([[1], [2], [3]])  # column matrix\nw = np.array([4, 5, 6])  # row matrix\nb = -1.5\n\n# Test with different transfer functions\ntransfer_functions = ['linear', 'logsigmoid', 'relu', 'hardlimit', 'tanh']\n\nfor f in transfer_functions:\n    output = neuron(p, w, b, f)\n    print(\"Transfer function:\", f)\n    print(\"Output (a):\", float(output))\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T20:20:04.575278Z","iopub.execute_input":"2023-09-16T20:20:04.575699Z","iopub.status.idle":"2023-09-16T20:20:04.589289Z","shell.execute_reply.started":"2023-09-16T20:20:04.575665Z","shell.execute_reply":"2023-09-16T20:20:04.588067Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Transfer function: linear\nOutput (a): 30.5\n\nTransfer function: logsigmoid\nOutput (a): 0.9999999999999432\n\nTransfer function: relu\nOutput (a): 30.5\n\nTransfer function: hardlimit\nOutput (a): 1.0\n\nTransfer function: tanh\nOutput (a): 1.0\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Q2 What the output ‘a’ of the multiple input neuron if  p=[1 2 3], w=[4 5 6], b=-1.5 and transfer function is \n# ‘poslin’? ","metadata":{}},{"cell_type":"markdown","source":"# Check if the answer that you have given above is correct?  \n# \n# positive linearl / Relu =30.5\n# because\n# [1]\n# [2]   dot  [4,5,6]  + (-1.5)   =1*4+ 2*5 + 3*6   30.5\n# [3]\n# 3 x 1 and 1 x 3     where     1=1\n# \n# the condition is :\n# n ≥ 0, relu(n) = n\n# else\n# n < 0, relu(n) = 0 ","metadata":{}}]}